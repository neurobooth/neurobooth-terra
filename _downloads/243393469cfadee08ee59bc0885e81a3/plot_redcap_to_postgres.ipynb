{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Ingest table from Redcap into Postgres\n\nThis example demonstrates how to create table from Redcap.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Mainak Jas <mjas@harvard.mgh.edu>\n\nimport os\n\nfrom redcap import Project, RedcapError\nfrom neurobooth_terra.redcap import (fetch_survey, iter_interval,\n                                     compare_dataframes,\n                                     combine_indicator_columns)\n\nimport psycopg2\nfrom sshtunnel import SSHTunnelForwarder\n\nfrom neurobooth_terra import Table, create_table, drop_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ssh arguments and connection arguments\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ssh_args = dict(\n        ssh_address_or_host='neurodoor.nmr.mgh.harvard.edu',\n        ssh_username='mj513',\n        ssh_config_file='~/.ssh/config',\n        ssh_pkey='~/.ssh/id_rsa',\n        remote_bind_address=('192.168.100.1', 5432),\n        local_bind_address=('localhost', 6543)\n)\n\ndb_args = dict(\n    database='neurobooth', user='neuroboother', password='neuroboothrocks',\n    # host='localhost'\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us first define the surveys and their survey IDs that we want to fetch.\nThis information can be found on Redcap. To fetch Redcap data, you will\nalso need to define the NEUROBOOTH_REDCAP_TOKEN environment variable.\nYou will need to request for the Redcap API token from Redcap interface.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "survey_ids = {'consent': 84349, 'contact': 84427, 'demographics': 84429,\n              'clinical': 84431, 'falls': 85031, 'subject': 96398}\nsurvey_ids = {'subject': 96397, 'consent': 96398, 'demographics': 84429,\n              'clinical': 84431}\n\nURL = 'https://redcap.partners.org/redcap/api/'\nAPI_KEY = os.environ.get('NEUROBOOTH_REDCAP_TOKEN')\n\nif API_KEY is None:\n    raise ValueError('Please define the environment variable NEUROBOOTH_REDCAP_TOKEN first')\n\nproject = Project(URL, API_KEY, lazy=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we fetch the metadata table. This table is the master table\nthat contains columns and their informations. It can be used to infer\ninformation about the columns: example, what choices are available for a\nparticular question.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('Fetching metadata ...')\nmetadata = project.export_metadata(format='df')\nmetadata_fields = ['field_label', 'form_name', 'section_header',\n                   'field_type', 'select_choices_or_calculations',\n                   'required_field']\nmetadata = metadata[metadata_fields]\n# metadata.to_csv(op.join(data_dir, 'data_dictionary.csv'), index=False)\nprint('[Done]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we loop over the surveys and collect them.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\nimport hashlib\n\n# TODO: add test for iter_interval\nfor _ in iter_interval(wait=5, exit_after=2):\n\n    dfs = dict()\n    for survey_name, survey_id in survey_ids.items():\n        df = fetch_survey(project, survey_name, survey_id, index='record_id')\n        # convert NaN to None for psycopg2\n        dfs[survey_name] = df\n\n    mapping = {f'race___{v}': v for v in range(1, 8)}\n    dfs['demographics'] = combine_indicator_columns(\n        dfs['demographics'], mapping, 'race')\n\n    mapping = {f'ancestry_cateogry___{v}': v for v in range(1, 13)}\n    dfs['demographics'] = combine_indicator_columns(\n        dfs['demographics'], mapping, 'ancestry_category')\n\n    mapping = {f'health_history___{v}': v for v in range(1, 39)}\n    # mapping['health_history___diabetics'] = 39\n    dfs['demographics'] = combine_indicator_columns(\n        dfs['demographics'], mapping, 'health_history')\n\n    # Now, we will prepare the contents of the subject table in postgres\n    drop_rows = pd.isna(dfs['subject']['first_name_birth'])\n    drop_record_ids = dfs['subject'].index[drop_rows]\n\n    dfs['subject'] = dfs['subject'].drop(drop_record_ids)\n    dfs['consent'] = dfs['consent'].drop(drop_record_ids, errors='ignore')\n\n    rows_subject = list()\n    for record_id, df_row in dfs['subject'].iterrows():\n\n        mainak_hash = 'hash'\n\n        rows_subject.append((record_id,\n                             df_row['first_name_birth'],\n                             df_row['middle_name_birth'],\n                             df_row['last_name_birth'],\n                             df_row['date_of_birth'],\n                             df_row['country_of_birth'],\n                             df_row['gender_at_birth'],\n                             df_row['birthplace']))\n\n    rows_consent = list()\n    for record_id, df_row in dfs['consent'].iterrows():\n        rows_consent.append((record_id,\n                            'study1',  # study_id\n                            df_row['redcap_event_name'],\n                            'Neuroboother',  # staff_id\n                            'REDCAP',  # application_id\n                            'MGH',  # site_id\n                            # None, # date (missing)\n                            df_row['educate_clinicians'],\n                            df_row['educate_clinicians_initials'],\n                            # bool(df_row['future_research_consent_adult'])\n        ))\n\n    rows_demographics = list()\n    for record_id, df_row in dfs['demographics'].iterrows():\n        rows_demographics.append((record_id,\n                                  'study1',  # study_id\n                                  df_row['redcap_event_name'],\n                                  'REDCAP',  # application_id\n                                  df_row['gender'],\n                                  df_row['ethnicity'],\n                                  df_row['handedness'],\n                                  df_row['health_history'],\n                                  df_row['race']  \n        ))\n\n    for row_subject in rows_subject[:5]:\n        print(row_subject)\n\n    # Then we insert the rows in this table\n    cols_subject = ['subject_id', 'first_name_birth', 'middle_name_birth',\n                    'last_name_birth', 'date_of_birth', 'country_of_birth',\n                    'gender_at_birth', 'birthplace']\n    cols_consent = ['subject_id', 'study_id', 'event_name',\n                    'staff_id', 'application_id', 'site_id',\n                    'educate_clinicians', 'educate_clinicians_initials']\n    cols_demographics = ['subject_id', 'study_id', 'event_name', 'application_id',\n                         'gender', 'ethnicity', 'handedness', 'health_history', 'race']\n    with SSHTunnelForwarder(**ssh_args) as tunnel:\n        with psycopg2.connect(port=tunnel.local_bind_port,\n                              host=tunnel.local_bind_host, **db_args) as conn:\n\n            table_subject = Table('subject', conn)\n            table_consent = Table('consent', conn)\n            table_demographics = Table('demographics', conn)\n\n            df_subject_db = table_subject.query()\n            df_consent_db = table_consent.query()\n            compare_dataframes(dfs['subject'], df_subject_db)\n            # compare_dataframes(dfs['consent'], df_consent_db)\n\n            table_subject.insert_rows(rows_subject, cols_subject,\n                                      on_conflict='update')\n            table_consent.insert_rows(rows_consent, cols_consent,\n                                      on_conflict='update')\n            table_demographics.insert_rows(rows_demographics, cols_demographics,\n                                           on_conflict='update')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will drop our tables if they already exist\nthis is just for convenience so we can re-run this script\nand create a new mock subject table and consent table to test our script\ndrop_table('subject', conn)\ndrop_table('consent', conn)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# table_id = 'subject'\n# datatypes = ['VARCHAR (255)', 'VARCHAR (255)', 'VARCHAR (255)', 'VARCHAR (255)',\n#              'date', 'VARCHAR (255)', 'VARCHAR (255)', 'VARCHAR (255)']\n# table_subject = create_table(table_id, conn, cols_subject, datatypes)\n\n# table_id = 'consent'\n# datatypes = ['VARCHAR (255)'] * len(cols_consent)\n# table_consent = create_table(table_id, conn, cols_consent, datatypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's do a query to check that the content is in there\nprint(table_subject.query())\nprint(table_consent.query())\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}